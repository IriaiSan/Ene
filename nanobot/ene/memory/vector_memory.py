"""VectorMemory — Ene's long-term memory backed by ChromaDB.

Three collections:
- memories: facts, archived core entries, diary excerpts, reflections
- entities: people, places, projects Ene has interacted with
- reflections: higher-level insights generated by the sleep agent

Retrieval uses three-factor scoring:
  score = (cosine_similarity * 0.5) + (recency * 0.25) + (importance/10 * 0.25)

Memory strength (Ebbinghaus-inspired decay):
  strength = max(0.1, exp(-decay_rate * hours / max(access_count * 5, 1)))
"""

from __future__ import annotations

import uuid
import math
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Callable

import chromadb
from loguru import logger


# ── Data Types ─────────────────────────────────────────────


@dataclass
class MemoryResult:
    """A single memory search result with scoring details."""

    id: str
    content: str
    memory_type: str
    importance: int
    score: float  # combined three-factor score
    distance: float  # raw cosine distance from ChromaDB
    created_at: str
    last_accessed_at: str
    access_count: int
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class EntityResult:
    """An entity search result."""

    id: str
    name: str
    entity_type: str
    description: str
    importance: int
    interaction_count: int
    first_seen: str
    last_seen: str
    aliases: str  # comma-separated
    metadata: dict[str, Any] = field(default_factory=dict)


# ── Constants ──────────────────────────────────────────────

MEMORY_TYPES = {"fact", "diary", "reflection", "archived_core"}
ENTITY_TYPES = {"person", "place", "project", "organization", "other"}
DEFAULT_DECAY_RATE = 0.1
DEFAULT_PRUNE_THRESHOLD = 0.2


# ── VectorMemory ───────────────────────────────────────────


class VectorMemory:
    """Long-term vector memory backed by ChromaDB.

    Manages three collections for memories, entities, and reflections.
    Supports three-factor retrieval scoring and Ebbinghaus decay.
    """

    def __init__(
        self,
        chroma_path: str | None = None,
        embedding_fn: Callable[[list[str]], list[list[float]]] | None = None,
        client: chromadb.ClientAPI | None = None,
    ):
        """Initialize VectorMemory.

        Args:
            chroma_path: Path for PersistentClient. Ignored if client is provided.
            embedding_fn: Function that takes list[str] and returns list[list[float]].
                If None, ChromaDB's default embedding will be used.
            client: Optional pre-configured ChromaDB client (for testing with in-memory).
        """
        if client is not None:
            self._client = client
        elif chroma_path:
            self._client = chromadb.PersistentClient(path=chroma_path)
        else:
            self._client = chromadb.Client()  # ephemeral, for testing

        self._embed_fn = embedding_fn

        # Get or create collections
        self._memories = self._client.get_or_create_collection(
            name="memories",
            metadata={"hnsw:space": "cosine"},
        )
        self._entities = self._client.get_or_create_collection(
            name="entities",
            metadata={"hnsw:space": "cosine"},
        )
        self._reflections = self._client.get_or_create_collection(
            name="reflections",
            metadata={"hnsw:space": "cosine"},
        )

    # ── Memories ───────────────────────────────────────────

    def add_memory(
        self,
        content: str,
        memory_type: str = "fact",
        importance: int = 5,
        source: str = "",
        related_entities: str = "",
    ) -> str:
        """Add a memory to the vector store.

        Args:
            content: The memory content text.
            memory_type: One of: fact, diary, reflection, archived_core.
            importance: 1-10 importance score.
            source: Where the memory came from (e.g., "discord", "sleep_agent").
            related_entities: Comma-separated entity names.

        Returns:
            Memory ID.
        """
        memory_id = uuid.uuid4().hex[:8]
        now = datetime.now().isoformat(timespec="seconds")
        importance = max(1, min(10, importance))

        metadata = {
            "type": memory_type,
            "importance": importance,
            "source": source,
            "created_at": now,
            "last_accessed_at": now,
            "access_count": 0,
            "related_entities": related_entities,
            "superseded_by": "",
        }

        add_kwargs: dict[str, Any] = {
            "ids": [memory_id],
            "documents": [content],
            "metadatas": [metadata],
        }

        if self._embed_fn:
            vectors = self._embed_fn([content])
            add_kwargs["embeddings"] = vectors

        self._memories.add(**add_kwargs)
        logger.debug(f"Added memory [{memory_id}] type={memory_type}: {content[:60]}")
        return memory_id

    def search(
        self,
        query: str,
        memory_type: str | None = None,
        limit: int = 5,
        min_importance: int = 0,
        overfetch_factor: int = 4,
        decay_rate: float = DEFAULT_DECAY_RATE,
    ) -> list[MemoryResult]:
        """Search memories with three-factor scoring.

        Score = (cosine_similarity * 0.5) + (recency * 0.25) + (importance/10 * 0.25)

        Args:
            query: Search query text.
            memory_type: Filter by type (None = all types).
            limit: Max results to return.
            min_importance: Filter out memories below this importance.
            overfetch_factor: Fetch this many times `limit` for reranking.
            decay_rate: Decay rate for recency calculation.

        Returns:
            List of MemoryResult, sorted by score descending.
        """
        n_fetch = limit * overfetch_factor

        # Build where filter
        where_filter: dict[str, Any] | None = None
        conditions: list[dict] = []

        if memory_type:
            conditions.append({"type": {"$eq": memory_type}})
        if min_importance > 0:
            conditions.append({"importance": {"$gte": min_importance}})
        # Exclude superseded memories
        conditions.append({"superseded_by": {"$eq": ""}})

        if len(conditions) == 1:
            where_filter = conditions[0]
        elif len(conditions) > 1:
            where_filter = {"$and": conditions}

        query_kwargs: dict[str, Any] = {
            "n_results": min(n_fetch, max(self._memories.count(), 1)),
        }
        if where_filter:
            query_kwargs["where"] = where_filter

        if self._embed_fn:
            vectors = self._embed_fn([query])
            query_kwargs["query_embeddings"] = vectors
        else:
            query_kwargs["query_texts"] = [query]

        try:
            results = self._memories.query(**query_kwargs)
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []

        if not results["ids"] or not results["ids"][0]:
            return []

        # Rerank with three-factor scoring
        now = datetime.now()
        scored: list[MemoryResult] = []

        for i, mid in enumerate(results["ids"][0]):
            meta = results["metadatas"][0][i] if results["metadatas"] else {}
            doc = results["documents"][0][i] if results["documents"] else ""
            dist = results["distances"][0][i] if results["distances"] else 0.0

            # ChromaDB cosine distance: lower = more similar
            # Convert to similarity: sim = 1 - distance (for cosine)
            # But ChromaDB returns negative cosine distance, so sim = -distance
            cosine_sim = max(0.0, min(1.0, -dist if dist < 0 else 1.0 - dist))

            # Recency: linear decay over 30 days
            created_str = meta.get("last_accessed_at", meta.get("created_at", ""))
            try:
                last_access = datetime.fromisoformat(created_str)
                days_ago = (now - last_access).total_seconds() / 86400
                recency = max(0.0, 1.0 - (days_ago / 30.0))
            except (ValueError, TypeError):
                recency = 0.5

            importance = meta.get("importance", 5)
            score = (cosine_sim * 0.5) + (recency * 0.25) + (importance / 10.0 * 0.25)

            scored.append(
                MemoryResult(
                    id=mid,
                    content=doc,
                    memory_type=meta.get("type", "fact"),
                    importance=importance,
                    score=score,
                    distance=dist,
                    created_at=meta.get("created_at", ""),
                    last_accessed_at=meta.get("last_accessed_at", ""),
                    access_count=meta.get("access_count", 0),
                    metadata=meta,
                )
            )

        # Sort by score descending, return top-k
        scored.sort(key=lambda r: r.score, reverse=True)
        top = scored[:limit]

        # Update access metadata for returned results
        for result in top:
            self._update_access(result.id)

        return top

    def _update_access(self, memory_id: str) -> None:
        """Bump last_accessed_at and access_count for a memory."""
        try:
            existing = self._memories.get(ids=[memory_id])
            if not existing["ids"]:
                return
            meta = existing["metadatas"][0] if existing["metadatas"] else {}
            meta["last_accessed_at"] = datetime.now().isoformat(timespec="seconds")
            meta["access_count"] = meta.get("access_count", 0) + 1
            self._memories.update(ids=[memory_id], metadatas=[meta])
        except Exception as e:
            logger.warning(f"Failed to update access for {memory_id}: {e}")

    def update_access(self, memory_id: str) -> None:
        """Public API to update access metadata."""
        self._update_access(memory_id)

    def mark_superseded(self, old_id: str, new_id: str) -> None:
        """Mark a memory as superseded by a newer one (contradiction resolution).

        Superseded memories are excluded from search results.
        """
        try:
            existing = self._memories.get(ids=[old_id])
            if not existing["ids"]:
                return
            meta = existing["metadatas"][0] if existing["metadatas"] else {}
            meta["superseded_by"] = new_id
            self._memories.update(ids=[old_id], metadatas=[meta])
            logger.info(f"Memory [{old_id}] superseded by [{new_id}]")
        except Exception as e:
            logger.error(f"Failed to mark superseded {old_id}: {e}")

    def get_memory(self, memory_id: str) -> dict | None:
        """Get a single memory by ID."""
        try:
            result = self._memories.get(ids=[memory_id])
            if not result["ids"]:
                return None
            return {
                "id": result["ids"][0],
                "content": result["documents"][0] if result["documents"] else "",
                "metadata": result["metadatas"][0] if result["metadatas"] else {},
            }
        except Exception:
            return None

    def delete_memory(self, memory_id: str) -> bool:
        """Permanently delete a memory.

        Returns True if deleted, False if not found.
        """
        try:
            existing = self._memories.get(ids=[memory_id])
            if not existing["ids"]:
                return False
            self._memories.delete(ids=[memory_id])
            logger.info(f"Permanently deleted memory [{memory_id}]")
            return True
        except Exception as e:
            logger.error(f"Failed to delete memory {memory_id}: {e}")
            return False

    def get_pruning_candidates(
        self,
        decay_rate: float = DEFAULT_DECAY_RATE,
        prune_threshold: float = DEFAULT_PRUNE_THRESHOLD,
        max_importance: int = 4,
        limit: int = 20,
    ) -> list[dict]:
        """Find memories that are candidates for pruning (weak + unimportant).

        Uses Ebbinghaus decay:
          strength = max(0.1, exp(-decay_rate * hours / max(access_count * 5, 1)))

        Returns memories where strength < prune_threshold AND importance <= max_importance.
        """
        try:
            # Get all memories with low importance
            results = self._memories.get(
                where={"importance": {"$lte": max_importance}},
            )
        except Exception as e:
            logger.error(f"Failed to get pruning candidates: {e}")
            return []

        if not results["ids"]:
            return []

        now = datetime.now()
        candidates = []

        for i, mid in enumerate(results["ids"]):
            meta = results["metadatas"][i] if results["metadatas"] else {}

            # Calculate decay strength
            last_access_str = meta.get("last_accessed_at", meta.get("created_at", ""))
            try:
                last_access = datetime.fromisoformat(last_access_str)
                hours = (now - last_access).total_seconds() / 3600
            except (ValueError, TypeError):
                hours = 720  # default to 30 days if unknown

            access_count = meta.get("access_count", 0)
            strength = max(
                0.1,
                math.exp(-decay_rate * hours / max(access_count * 5, 1)),
            )

            if strength < prune_threshold:
                candidates.append({
                    "id": mid,
                    "content": results["documents"][i] if results["documents"] else "",
                    "importance": meta.get("importance", 5),
                    "strength": round(strength, 3),
                    "access_count": access_count,
                    "last_accessed_at": last_access_str,
                    "metadata": meta,
                })

        # Sort by strength ascending (weakest first)
        candidates.sort(key=lambda c: c["strength"])
        return candidates[:limit]

    def get_memory_count(self) -> int:
        """Total number of memories in the store."""
        return self._memories.count()

    # ── Entities ───────────────────────────────────────────

    def add_entity(
        self,
        name: str,
        entity_type: str = "person",
        description: str = "",
        importance: int = 5,
        aliases: str = "",
    ) -> str:
        """Add a new entity to the entity collection.

        Args:
            name: Entity name (e.g., "CCC", "Discord Server").
            entity_type: One of: person, place, project, organization, other.
            description: Free-text description of the entity.
            importance: 1-10 importance score.
            aliases: Comma-separated alternative names.

        Returns:
            Entity ID.
        """
        entity_id = uuid.uuid4().hex[:8]
        now = datetime.now().isoformat(timespec="seconds")

        metadata = {
            "entity_type": entity_type,
            "name": name,
            "importance": max(1, min(10, importance)),
            "aliases": aliases,
            "first_seen": now,
            "last_seen": now,
            "interaction_count": 1,
        }

        # Use "name: description" as the document for embedding
        doc = f"{name}: {description}" if description else name

        add_kwargs: dict[str, Any] = {
            "ids": [entity_id],
            "documents": [doc],
            "metadatas": [metadata],
        }

        if self._embed_fn:
            vectors = self._embed_fn([doc])
            add_kwargs["embeddings"] = vectors

        self._entities.add(**add_kwargs)
        logger.debug(f"Added entity [{entity_id}] {name} ({entity_type})")
        return entity_id

    def get_entity_by_name(self, name: str) -> EntityResult | None:
        """Look up an entity by exact name match.

        Also checks aliases.
        """
        try:
            # Search by name in metadata
            results = self._entities.get(
                where={"name": {"$eq": name}},
            )
            if results["ids"]:
                return self._to_entity_result(
                    results["ids"][0],
                    results["documents"][0] if results["documents"] else "",
                    results["metadatas"][0] if results["metadatas"] else {},
                )

            # If not found by name, try to find in aliases
            # ChromaDB doesn't support substring matching, so get all and check
            all_entities = self._entities.get()
            if all_entities["ids"]:
                for i, eid in enumerate(all_entities["ids"]):
                    meta = all_entities["metadatas"][i] if all_entities["metadatas"] else {}
                    aliases = meta.get("aliases", "")
                    if name.lower() in [a.strip().lower() for a in aliases.split(",") if a.strip()]:
                        return self._to_entity_result(
                            eid,
                            all_entities["documents"][i] if all_entities["documents"] else "",
                            meta,
                        )
        except Exception as e:
            logger.error(f"Failed to look up entity '{name}': {e}")

        return None

    def upsert_entity(
        self,
        name: str,
        description: str | None = None,
        importance: int | None = None,
        entity_type: str | None = None,
        aliases: str | None = None,
    ) -> str:
        """Update an existing entity or create a new one.

        Returns the entity ID (existing or new).
        """
        existing = self.get_entity_by_name(name)

        if existing:
            # Update existing
            meta = existing.metadata.copy()
            meta["last_seen"] = datetime.now().isoformat(timespec="seconds")
            meta["interaction_count"] = existing.interaction_count + 1

            if importance is not None:
                meta["importance"] = max(1, min(10, importance))
            if entity_type is not None:
                meta["entity_type"] = entity_type
            if aliases is not None:
                meta["aliases"] = aliases

            update_kwargs: dict[str, Any] = {
                "ids": [existing.id],
                "metadatas": [meta],
            }

            if description is not None:
                doc = f"{name}: {description}"
                update_kwargs["documents"] = [doc]
                if self._embed_fn:
                    update_kwargs["embeddings"] = self._embed_fn([doc])

            self._entities.update(**update_kwargs)
            logger.debug(f"Updated entity [{existing.id}] {name}")
            return existing.id
        else:
            # Create new
            return self.add_entity(
                name=name,
                entity_type=entity_type or "person",
                description=description or "",
                importance=importance or 5,
                aliases=aliases or "",
            )

    def search_entities(
        self,
        query: str,
        entity_type: str | None = None,
        limit: int = 5,
    ) -> list[EntityResult]:
        """Search entities by semantic similarity.

        Args:
            query: Search text.
            entity_type: Filter by type (None = all).
            limit: Max results.

        Returns:
            List of EntityResult.
        """
        query_kwargs: dict[str, Any] = {
            "n_results": min(limit, max(self._entities.count(), 1)),
        }

        if entity_type:
            query_kwargs["where"] = {"entity_type": {"$eq": entity_type}}

        if self._embed_fn:
            vectors = self._embed_fn([query])
            query_kwargs["query_embeddings"] = vectors
        else:
            query_kwargs["query_texts"] = [query]

        try:
            results = self._entities.query(**query_kwargs)
        except Exception as e:
            logger.error(f"Entity search failed: {e}")
            return []

        if not results["ids"] or not results["ids"][0]:
            return []

        entities = []
        for i, eid in enumerate(results["ids"][0]):
            meta = results["metadatas"][0][i] if results["metadatas"] else {}
            doc = results["documents"][0][i] if results["documents"] else ""
            entities.append(self._to_entity_result(eid, doc, meta))

        return entities

    def get_entity_names(self) -> dict[str, str]:
        """Get all entity names → IDs mapping.

        Used for keyword matching cache.
        Returns dict mapping lowercase name → entity ID.
        """
        try:
            all_entities = self._entities.get()
        except Exception:
            return {}

        names: dict[str, str] = {}
        if not all_entities["ids"]:
            return names

        for i, eid in enumerate(all_entities["ids"]):
            meta = all_entities["metadatas"][i] if all_entities["metadatas"] else {}
            name = meta.get("name", "")
            if name:
                names[name.lower()] = eid
            # Also add aliases
            aliases = meta.get("aliases", "")
            for alias in aliases.split(","):
                alias = alias.strip()
                if alias:
                    names[alias.lower()] = eid

        return names

    def get_entity_count(self) -> int:
        """Total number of entities."""
        return self._entities.count()

    def _to_entity_result(self, eid: str, doc: str, meta: dict) -> EntityResult:
        """Convert raw data to EntityResult."""
        return EntityResult(
            id=eid,
            name=meta.get("name", ""),
            entity_type=meta.get("entity_type", "other"),
            description=doc,
            importance=meta.get("importance", 5),
            interaction_count=meta.get("interaction_count", 0),
            first_seen=meta.get("first_seen", ""),
            last_seen=meta.get("last_seen", ""),
            aliases=meta.get("aliases", ""),
            metadata=meta,
        )

    # ── Reflections ────────────────────────────────────────

    def add_reflection(
        self,
        content: str,
        importance: int = 5,
        source_ids: str = "",
        topic: str = "",
    ) -> str:
        """Add a reflection (higher-level insight from the sleep agent).

        Args:
            content: The reflection text.
            importance: 1-10 importance score.
            source_ids: Comma-separated memory IDs this reflection is based on.
            topic: Topic label for the reflection.

        Returns:
            Reflection ID.
        """
        ref_id = uuid.uuid4().hex[:8]
        now = datetime.now().isoformat(timespec="seconds")

        metadata = {
            "created_at": now,
            "source_memory_ids": source_ids,
            "importance": max(1, min(10, importance)),
            "topic": topic,
        }

        add_kwargs: dict[str, Any] = {
            "ids": [ref_id],
            "documents": [content],
            "metadatas": [metadata],
        }

        if self._embed_fn:
            vectors = self._embed_fn([content])
            add_kwargs["embeddings"] = vectors

        self._reflections.add(**add_kwargs)
        logger.debug(f"Added reflection [{ref_id}]: {content[:60]}")
        return ref_id

    def search_reflections(
        self,
        query: str,
        limit: int = 3,
    ) -> list[dict]:
        """Search reflections by semantic similarity.

        Returns list of dicts with id, content, importance, topic, created_at.
        """
        query_kwargs: dict[str, Any] = {
            "n_results": min(limit, max(self._reflections.count(), 1)),
        }

        if self._embed_fn:
            vectors = self._embed_fn([query])
            query_kwargs["query_embeddings"] = vectors
        else:
            query_kwargs["query_texts"] = [query]

        try:
            results = self._reflections.query(**query_kwargs)
        except Exception as e:
            logger.error(f"Reflection search failed: {e}")
            return []

        if not results["ids"] or not results["ids"][0]:
            return []

        reflections = []
        for i, rid in enumerate(results["ids"][0]):
            meta = results["metadatas"][0][i] if results["metadatas"] else {}
            doc = results["documents"][0][i] if results["documents"] else ""
            reflections.append({
                "id": rid,
                "content": doc,
                "importance": meta.get("importance", 5),
                "topic": meta.get("topic", ""),
                "created_at": meta.get("created_at", ""),
                "source_memory_ids": meta.get("source_memory_ids", ""),
            })

        return reflections

    def get_reflection_count(self) -> int:
        """Total number of reflections."""
        return self._reflections.count()
